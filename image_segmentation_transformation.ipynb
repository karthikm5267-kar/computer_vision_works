# ==============================================
# IMAGE OBJECT SEGMENTATION & TRANSFORMATION
# Google Colab Ready Version
# ==============================================

#@title **Step 1: Install Required Libraries**
!pip install opencv-python numpy matplotlib Pillow scikit-image -q
print("‚úÖ Libraries installed successfully!")

#@title **Step 2: Import Libraries**
import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os
import urllib.request
from google.colab import files
from IPython.display import display, HTML
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Libraries imported!")

#@title **Step 3: Upload Your Image**
print("üì§ Please upload your image file...")
uploaded = files.upload()

# Get the uploaded filename
image_filename = list(uploaded.keys())[0] if uploaded else None

if image_filename:
    print(f"‚úÖ Uploaded: {image_filename}")
else:
    print("‚ùå No file uploaded. Please run this cell again.")
    # Fallback: use a sample image
    sample_url = "https://images.unsplash.com/photo-1524758631624-e2822e304c36?ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80"
    image_filename = "sample_image.jpg"
    urllib.request.urlretrieve(sample_url, image_filename)
    print(f"üì• Using sample image: {image_filename}")

#@title **Step 4: Load and Display Image**
def load_image(image_path):
    """Load image using PIL (handles all formats)"""
    try:
        img = Image.open(image_path)
        # Convert to RGB if necessary
        if img.mode != 'RGB':
            img = img.convert('RGB')
        return np.array(img)
    except Exception as e:
        print(f"Error loading image: {e}")
        return None

# Load the image
image_rgb = load_image(image_filename)

if image_rgb is not None:
    # Display original image
    plt.figure(figsize=(10, 8))
    plt.imshow(image_rgb)
    plt.title(f'Original Image: {image_filename}\nSize: {image_rgb.shape[1]}x{image_rgb.shape[0]}')
    plt.axis('off')
    plt.show()
    
    # Convert to BGR for OpenCV operations
    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
else:
    print("‚ùå Failed to load image")
    image_bgr = None

#@title **Step 5: Object Detection Functions**
def segment_objects_contour(image_bgr):
    """Segment objects using contour detection"""
    # Convert to grayscale
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    
    # Apply adaptive thresholding
    thresh = cv2.adaptiveThreshold(gray, 255, 
                                   cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                   cv2.THRESH_BINARY_INV, 11, 2)
    
    # Apply morphological operations
    kernel = np.ones((3, 3), np.uint8)
    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)
    
    # Find contours
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    objects = []
    h, w = image_bgr.shape[:2]
    
    for i, cnt in enumerate(contours):
        area = cv2.contourArea(cnt)
        
        # Filter by area
        min_area = (h * w) * 0.001  # 0.1% of image area
        max_area = (h * w) * 0.5    # 50% of image area
        
        if area < min_area or area > max_area:
            continue
        
        # Get bounding box
        x, y, width, height = cv2.boundingRect(cnt)
        
        # Filter by size
        if width < 30 or height < 30:
            continue
        
        # Calculate aspect ratio
        aspect_ratio = width / float(height)
        
        # Skip very narrow or very wide objects
        if aspect_ratio < 0.2 or aspect_ratio > 5:
            continue
        
        # Extract object
        obj_img = image_bgr[y:y+height, x:x+width]
        
        if obj_img.size == 0:
            continue
        
        # Create mask
        mask = np.zeros((height, width), dtype=np.uint8)
        cnt_shifted = cnt - np.array([x, y])
        cv2.drawContours(mask, [cnt_shifted], -1, 255, -1)
        
        # Apply mask to object
        obj_masked = cv2.bitwise_and(obj_img, obj_img, mask=mask)
        
        # Determine object type based on shape
        if aspect_ratio > 1.5:
            label = "horizontal"
        elif aspect_ratio < 0.7:
            label = "vertical"
        elif 0.8 < aspect_ratio < 1.2:
            label = "square"
        else:
            label = "object"
        
        objects.append({
            'id': i,
            'image': obj_masked,
            'mask': mask,
            'bbox': (x, y, width, height),
            'label': f"{label}_{i+1}",
            'score': min(area / 5000, 0.95),  # Pseudo confidence
            'area': area,
            'aspect_ratio': aspect_ratio
        })
    
    # Sort by area (largest first)
    objects.sort(key=lambda x: x['area'], reverse=True)
    
    return objects

def segment_objects_color_based(image_bgr):
    """Segment objects using color-based segmentation"""
    # Convert to HSV color space
    hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)
    
    # Define color ranges for different objects
    color_ranges = [
        ([0, 50, 50], [10, 255, 255], "red_object"),    # Red
        ([25, 50, 50], [35, 255, 255], "yellow_object"), # Yellow
        ([100, 50, 50], [130, 255, 255], "blue_object"), # Blue
        ([35, 50, 50], [85, 255, 255], "green_object"),  # Green
    ]
    
    objects = []
    object_id = 0
    
    for (lower, upper, label) in color_ranges:
        # Create mask for color range
        mask = cv2.inRange(hsv, np.array(lower), np.array(upper))
        
        # Apply morphological operations
        kernel = np.ones((5, 5), np.uint8)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        
        # Find contours
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for cnt in contours:
            area = cv2.contourArea(cnt)
            if area < 500:  # Minimum area threshold
                continue
            
            x, y, width, height = cv2.boundingRect(cnt)
            
            if width < 30 or height < 30:
                continue
            
            # Extract object
            obj_img = image_bgr[y:y+height, x:x+width]
            obj_mask = mask[y:y+height, x:x+width]
            
            # Apply mask
            obj_masked = cv2.bitwise_and(obj_img, obj_img, mask=obj_mask)
            
            objects.append({
                'id': object_id,
                'image': obj_masked,
                'mask': obj_mask,
                'bbox': (x, y, width, height),
                'label': label,
                'score': min(area / 3000, 0.9),
                'area': area
            })
            object_id += 1
    
    return objects

#@title **Step 6: Transformation Functions**
def apply_all_transformations(obj_img):
    """Apply all transformations to an object"""
    if obj_img is None or obj_img.size == 0:
        return {}
    
    h, w = obj_img.shape[:2]
    
    # Dictionary to store transformations
    transformations = {}
    
    # 1. Original
    transformations['Original'] = obj_img.copy()
    
    # 2. Translation
    if w > 0 and h > 0:
        M = np.float32([[1, 0, 20], [0, 1, 15]])
        translated = cv2.warpAffine(obj_img, M, (w, h))
        transformations['Translation'] = translated
    
    # 3. Rotation (45 degrees)
    if w > 0 and h > 0:
        center = (w // 2, h // 2)
        M = cv2.getRotationMatrix2D(center, 45, 1.0)
        rotated = cv2.warpAffine(obj_img, M, (w, h))
        transformations['Rotation'] = rotated
    
    # 4. Scaling (1.5x)
    if w > 0 and h > 0:
        scaled = cv2.resize(obj_img, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_LINEAR)
        transformations['Scaling'] = scaled
    
    # 5. Shearing
    if w > 0 and h > 0:
        M = np.float32([[1, 0.3, 0], [0, 1, 0]])
        sheared = cv2.warpAffine(obj_img, M, (int(w * 1.3), h))
        transformations['Shearing'] = sheared
    
    # 6. Flipping (Horizontal)
    if w > 0 and h > 0:
        flip_h = cv2.flip(obj_img, 1)
        transformations['Flip_H'] = flip_h
    
    # 7. Flipping (Vertical)
    if w > 0 and h > 0:
        flip_v = cv2.flip(obj_img, 0)
        transformations['Flip_V'] = flip_v
    
    # 8. Affine Transformation
    if w > 50 and h > 50:  # Only for larger objects
        pts1 = np.float32([[0, 0], [w-1, 0], [0, h-1]])
        pts2 = np.float32([[10, 10], [w-20, 20], [20, h-20]])
        M = cv2.getAffineTransform(pts1, pts2)
        affine = cv2.warpAffine(obj_img, M, (w, h))
        transformations['Affine'] = affine
    
    return transformations

def create_transformation_grid(transformations, object_label):
    """Create a grid of transformations for display"""
    if not transformations:
        return None
    
    # Get list of transformation names and images
    trans_names = list(transformations.keys())
    trans_images = list(transformations.values())
    
    # Determine grid layout
    n_trans = len(trans_names)
    n_cols = min(4, n_trans)
    n_rows = (n_trans + n_cols - 1) // n_cols
    
    # Create figure
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))
    
    if n_rows == 1 and n_cols == 1:
        axes = np.array([axes])
    
    axes = axes.flatten()
    
    for i, (name, img) in enumerate(zip(trans_names, trans_images)):
        if i < len(axes):
            ax = axes[i]
            if len(img.shape) == 3:
                ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            else:
                ax.imshow(img, cmap='gray')
            ax.set_title(f"{name}")
            ax.axis('off')
    
    # Hide unused subplots
    for i in range(len(trans_names), len(axes)):
        axes[i].axis('off')
    
    plt.suptitle(f'Transformations: {object_label}', fontsize=16, y=1.02)
    plt.tight_layout()
    return fig

#@title **Step 7: Detect Objects in Image**
print("üîç Detecting objects...")

if image_bgr is not None:
    # Try contour-based segmentation
    objects_contour = segment_objects_contour(image_bgr)
    
    # Try color-based segmentation
    objects_color = segment_objects_color_based(image_bgr)
    
    # Combine both methods
    all_objects = objects_contour + objects_color
    
    # Remove duplicates (based on overlapping bounding boxes)
    unique_objects = []
    used_areas = []
    
    for obj in all_objects:
        x, y, w, h = obj['bbox']
        obj_area = (x, y, x+w, y+h)
        
        # Check if this area significantly overlaps with any used area
        is_duplicate = False
        for used_area in used_areas:
            # Calculate overlap
            x1, y1, x2, y2 = obj_area
            ux1, uy1, ux2, uy2 = used_area
            
            # Compute intersection
            inter_x1 = max(x1, ux1)
            inter_y1 = max(y1, uy1)
            inter_x2 = min(x2, ux2)
            inter_y2 = min(y2, uy2)
            
            if inter_x1 < inter_x2 and inter_y1 < inter_y2:
                # Calculate overlap ratio
                inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                obj_total_area = w * h
                
                if inter_area > 0.7 * obj_total_area:  # 70% overlap threshold
                    is_duplicate = True
                    break
        
        if not is_duplicate:
            unique_objects.append(obj)
            used_areas.append(obj_area)
    
    print(f"‚úÖ Found {len(unique_objects)} unique objects")
    
    # Display detected objects
    if unique_objects:
        # Create image with bounding boxes
        img_with_boxes = image_bgr.copy()
        
        # Color palette for different objects
        colors = [
            (0, 255, 0),    # Green
            (255, 0, 0),    # Blue
            (0, 255, 255),  # Yellow
            (255, 0, 255),  # Magenta
            (255, 165, 0),  # Orange
            (128, 0, 128),  # Purple
            (0, 128, 128),  # Teal
            (128, 128, 0)   # Olive
        ]
        
        for i, obj in enumerate(unique_objects):
            x, y, w, h = obj['bbox']
            color = colors[i % len(colors)]
            
            # Draw bounding box
            cv2.rectangle(img_with_boxes, (x, y), (x+w, y+h), color, 3)
            
            # Draw label
            label = f"{obj['label']} ({obj['score']:.1f})"
            cv2.putText(img_with_boxes, label, (x, y-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        
        # Display results
        plt.figure(figsize=(12, 10))
        plt.imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))
        plt.title(f'Detected Objects: {len(unique_objects)} objects found', fontsize=16)
        plt.axis('off')
        plt.tight_layout()
        plt.show()
        
        # Save the detection image
        cv2.imwrite('detected_objects.jpg', img_with_boxes)
        files.download('detected_objects.jpg')
        print("üì• Downloaded: detected_objects.jpg")
    else:
        print("‚ùå No objects detected. Trying alternative approach...")
        
        # Alternative: Split image into a grid
        h, w = image_bgr.shape[:2]
        grid_rows, grid_cols = 2, 3
        
        unique_objects = []
        for i in range(grid_rows * grid_cols):
            row = i // grid_cols
            col = i % grid_cols
            
            cell_h = h // grid_rows
            cell_w = w // grid_cols
            
            y1 = row * cell_h
            y2 = min((row + 1) * cell_h, h)
            x1 = col * cell_w
            x2 = min((col + 1) * cell_w, w)
            
            obj_img = image_bgr[y1:y2, x1:x2]
            mask = np.ones((y2-y1, x2-x1), dtype=np.uint8) * 255
            
            unique_objects.append({
                'id': i,
                'image': obj_img,
                'mask': mask,
                'bbox': (x1, y1, x2-x1, y2-y1),
                'label': f'Region_{i+1}',
                'score': 0.5,
                'area': (x2-x1) * (y2-y1)
            })
        
        print(f"Created {len(unique_objects)} image regions")
else:
    unique_objects = []
    print("‚ùå Cannot detect objects - image not loaded")

#@title **Step 8: Apply Transformations to Each Object**
print("\nüîÑ Applying transformations to each object...")

all_transformations = []

if unique_objects:
    # Limit to first 6 objects for display
    display_objects = unique_objects[:6]
    
    for i, obj in enumerate(display_objects):
        print(f"\nProcessing object {i+1}: {obj['label']}")
        
        # Apply transformations
        transformations = apply_all_transformations(obj['image'])
        all_transformations.append(transformations)
        
        # Display transformations for this object
        if transformations:
            fig = create_transformation_grid(transformations, obj['label'])
            if fig:
                plt.show()
                plt.close(fig)
            
            # Save individual object
            cv2.imwrite(f'object_{i+1}_{obj["label"]}.jpg', obj['image'])
            print(f"  ‚úÖ Saved: object_{i+1}_{obj['label']}.jpg")
    
    print(f"\n‚úÖ Applied transformations to {len(display_objects)} objects")
else:
    print("‚ùå No objects to transform")

#@title **Step 9: Create Transformation Collage**
print("\nüé® Creating transformation collage...")

if all_transformations and len(all_transformations[0]) > 0:
    # Get transformation types from first object
    trans_types = list(all_transformations[0].keys())[:4]  # First 4 transformations
    
    # Create a collage for each transformation type
    for trans_type in trans_types:
        collage_images = []
        
        # Collect this transformation from each object
        for i, transformations in enumerate(all_transformations):
            if trans_type in transformations:
                img = transformations[trans_type]
                # Resize to consistent height
                h, w = img.shape[:2]
                target_h = 150
                target_w = int(w * (target_h / h))
                resized = cv2.resize(img, (target_w, target_h))
                collage_images.append(resized)
        
        # Create horizontal collage
        if collage_images:
            collage = np.hstack(collage_images)
            
            # Display collage
            plt.figure(figsize=(15, 5))
            plt.imshow(cv2.cvtColor(collage, cv2.COLOR_BGR2RGB))
            plt.title(f'{trans_type} Transformation - All Objects', fontsize=14)
            plt.axis('off')
            plt.tight_layout()
            plt.show()
            
            # Save collage
            collage_filename = f'{trans_type.lower()}_collage.jpg'
            cv2.imwrite(collage_filename, collage)
            files.download(collage_filename)
            print(f"üì• Downloaded: {collage_filename}")

#@title **Step 10: Save All Results**
print("\n Saving all results...")

# Create a summary text file
summary_lines = [
    "=" * 50,
    "IMAGE PROCESSING SUMMARY REPORT",
    "=" * 50,
    f"Image: {image_filename}",
    f"Original size: {image_rgb.shape[1]}x{image_rgb.shape[0]}",
    f"Objects detected: {len(unique_objects)}",
    "",
    "DETECTED OBJECTS:",
    "-" * 30
]

for i, obj in enumerate(unique_objects[:10]):  # List first 10 objects
    x, y, w, h = obj['bbox']
    summary_lines.append(f"Object {i+1}: {obj['label']}")
    summary_lines.append(f"  Position: ({x}, {y})")
    summary_lines.append(f"  Size: {w}x{h} pixels")
    summary_lines.append(f"  Area: {obj['area']:.0f} px¬≤")
    summary_lines.append(f"  Confidence: {obj['score']:.2f}")
    summary_lines.append("")

if len(unique_objects) > 10:
    summary_lines.append(f"... and {len(unique_objects) - 10} more objects")

summary_lines.extend([
    "",
    "=" * 50,
    "GENERATED FILES:",
    "-" * 30,
    "1. detected_objects.jpg - Image with bounding boxes",
    "2. object_1_[label].jpg - Individual object images",
    "3. [transformation]_collage.jpg - Transformation examples",
    "",
    "APPLIED TRANSFORMATIONS:",
    "-" * 30,
    "‚úì Translation",
    "‚úì Rotation",
    "‚úì Scaling",
    "‚úì Shearing",
    "‚úì Flipping (Horizontal & Vertical)",
    "‚úì Affine Transformation",
    "",
    "=" * 50
])

# Save summary to file
summary_text = "\n".join(summary_lines)
with open('processing_summary.txt', 'w') as f:
    f.write(summary_text)

# Download summary
files.download('processing_summary.txt')
print(" Downloaded: processing_summary.txt")

#@title **Step 11: Interactive Object Selection (Optional)**
print("\n INTERACTIVE OBJECT VIEWER")
print("Select an object to view in detail:")

if unique_objects:
    # Display all objects in a grid
    n_objects = min(len(unique_objects), 9)  # Show max 9 objects
    n_cols = 3
    n_rows = (n_objects + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    
    if n_rows == 1 and n_cols == 1:
        axes = np.array([axes])
    
    axes = axes.flatten()
    
    for i in range(n_objects):
        obj = unique_objects[i]
        obj_img = obj['image']
        
        ax = axes[i]
        if len(obj_img.shape) == 3:
            ax.imshow(cv2.cvtColor(obj_img, cv2.COLOR_BGR2RGB))
        else:
            ax.imshow(obj_img, cmap='gray')
        
        ax.set_title(f"Object {i+1}: {obj['label']}\n{obj_img.shape[1]}x{obj_img.shape[0]}")
        ax.axis('off')
    
    # Hide unused subplots
    for i in range(n_objects, len(axes)):
        axes[i].axis('off')
    
    plt.suptitle('Detected Objects (Click to select)', fontsize=16, y=1.02)
    plt.tight_layout()
    plt.show()
    
    # Allow user to select an object
    print("\nTo view a specific object, enter its number (1-{}) or 0 to skip:".format(n_objects))
    
    try:
        selection = int(input("Enter object number: "))
        
        if 1 <= selection <= n_objects:
            selected_obj = unique_objects[selection-1]
            print(f"\nSelected: Object {selection} - {selected_obj['label']}")
            print(f"Size: {selected_obj['image'].shape[1]}x{selected_obj['image'].shape[0]}")
            print(f"Position: {selected_obj['bbox'][:2]}")
            print(f"Area: {selected_obj['area']:.0f} pixels")
            
            # Apply and show detailed transformations
            transformations = apply_all_transformations(selected_obj['image'])
            
            if transformations:
                print("\nApplying all transformations...")
                fig = create_transformation_grid(transformations, selected_obj['label'])
                if fig:
                    plt.show()
                    plt.close(fig)
                
                # Save detailed view
                cv2.imwrite(f'selected_object_{selection}.jpg', selected_obj['image'])
                files.download(f'selected_object_{selection}.jpg')
                print(f"Downloaded: selected_object_{selection}.jpg")
    except:
        print("Invalid selection. Skipping interactive view.")
else:
    print("No objects to display.")

#@title **Step 12: Final Results Download**
print("\n" + "=" * 60)
print(" PROCESSING COMPLETE!")
print("=" * 60)

print("\n SUMMARY:")
print(f"- Image processed: {image_filename}")
print(f"- Objects detected: {len(unique_objects)}")
print(f"- Transformations applied: 7 types per object")

print("\n FILES GENERATED:")
print("1. detected_objects.jpg - Main detection result")
print("2. object_[n]_[label].jpg - Individual objects")
print("3. [transformation]_collage.jpg - Transformation collages")
print("4. processing_summary.txt - Detailed report")

print("\n  Download all files:")

# Create a list of all generated files
all_files = ['detected_objects.jpg', 'processing_summary.txt']

# Add object files
for i in range(min(len(unique_objects), 6)):
    all_files.append(f'object_{i+1}_{unique_objects[i]["label"]}.jpg')

# Add collage files
for trans_type in ['translation', 'rotation', 'scaling', 'shearing']:
    all_files.append(f'{trans_type}_collage.jpg')

print("\nReady to download! Check the files panel on the left.")

# Display completion message
print("\n" + "=" * 60)
print(" ALL OPERATIONS COMPLETED SUCCESSFULLY!")
print("=" * 60)
